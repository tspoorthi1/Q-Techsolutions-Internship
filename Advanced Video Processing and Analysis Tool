!pip install ultralytics -q
from google.colab import files

# Upload your video file
uploaded = files.upload() 
import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# Function to extract key frames
def extract_key_frames(video_path, threshold=30):
    cap = cv2.VideoCapture(video_path)
    prev_frame = None
    key_frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if prev_frame is not None:
            frame_diff = cv2.absdiff(prev_frame, gray_frame)
            mean_diff = np.mean(frame_diff)

            if mean_diff > threshold:
                key_frames.append(frame)

        prev_frame = gray_frame

    cap.release()
    return key_frames

# Function to detect objects using YOLOv8
def detect_objects(video_path):
    os.system(f"yolo detect predict model=yolov8m.pt source='{video_path}'")

# Function to track motion
def track_motion(video_path):
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()

    first_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)
    first_gray = cv2.GaussianBlur(first_gray, (5, 5), 0)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)

        frame_diff = cv2.absdiff(first_gray, gray_frame)
        _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)

        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for contour in contours:
            if cv2.contourArea(contour) < 500:
                continue
            (x, y, w, h) = cv2.boundingRect(contour)
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        cv2_imshow(frame)  # Display the frame

    cap.release()
import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# Function to extract key frames
def extract_key_frames(video_path, threshold=30):
    cap = cv2.VideoCapture(video_path)
    prev_frame = None
    key_frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if prev_frame is not None:
            frame_diff = cv2.absdiff(prev_frame, gray_frame)
            mean_diff = np.mean(frame_diff)

            if mean_diff > threshold:
                key_frames.append(frame)

        prev_frame = gray_frame

    cap.release()
    return key_frames

# Function to detect objects using YOLOv8
def detect_objects(video_path):
    os.system(f"yolo detect predict model=yolov8m.pt source='{video_path}'")

# Function to track motion
def track_motion(video_path):
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()

    first_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)
    first_gray = cv2.GaussianBlur(first_gray, (5, 5), 0)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)

        frame_diff = cv2.absdiff(first_gray, gray_frame)
        _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)

        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for contour in contours:
            if cv2.contourArea(contour) < 500:
                continue
            (x, y, w, h) = cv2.boundingRect(contour)
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        cv2_imshow(frame)  # Display the frame

    cap.release()
import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# Function to extract key frames
def extract_key_frames(video_path, threshold=30):
    cap = cv2.VideoCapture(video_path)
    prev_frame = None
    key_frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if prev_frame is not None:
            frame_diff = cv2.absdiff(prev_frame, gray_frame)
            mean_diff = np.mean(frame_diff)

            if mean_diff > threshold:
                key_frames.append(frame)

        prev_frame = gray_frame

    cap.release()
    return key_frames

# Function to detect objects using YOLOv8
def detect_objects(video_path):
    os.system(f"yolo detect predict model=yolov8m.pt source='{video_path}'")

# Function to track motion
def track_motion(video_path):
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()

    first_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)
    first_gray = cv2.GaussianBlur(first_gray, (5, 5), 0)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)

        frame_diff = cv2.absdiff(first_gray, gray_frame)
        _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)

        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for contour in contours:
            if cv2.contourArea(contour) < 500:
                continue
            (x, y, w, h) = cv2.boundingRect(contour)
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        cv2_imshow(frame)  # Display the frame

    cap.release()
import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# Function to extract key frames
def extract_key_frames(video_path, threshold=30):
    cap = cv2.VideoCapture(video_path)
    prev_frame = None
    key_frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if prev_frame is not None:
            frame_diff = cv2.absdiff(prev_frame, gray_frame)
            mean_diff = np.mean(frame_diff)

            if mean_diff > threshold:
                key_frames.append(frame)

        prev_frame = gray_frame

    cap.release()
    return key_frames

# Function to detect objects using YOLOv8
def detect_objects(video_path):
    os.system(f"yolo detect predict model=yolov8m.pt source='{video_path}'")

# Function to track motion
def track_motion(video_path):
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()

    first_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)
    first_gray = cv2.GaussianBlur(first_gray, (5, 5), 0)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)

        frame_diff = cv2.absdiff(first_gray, gray_frame)
        _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)

        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for contour in contours:
            if cv2.contourArea(contour) < 500:
                continue
            (x, y, w, h) = cv2.boundingRect(contour)
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        cv2_imshow(frame)  # Display the frame

    cap.release()
# Step 3: Define video path
video_path = '/content/video1.mp4'  # Replace with the uploaded file name

# Step 4: Extract key frames
key_frames = extract_key_frames(video_path)
print(f"Extracted {len(key_frames)} key frames.")

# Step 5: Detect objects in the video
detect_objects(video_path)

# Step 6: Track motion in the video
track_motion(video_path)
# Convert the output video to MP4 format
!ffmpeg -i "/content/runs/detect/predict3/your_video_file.avi" -vcodec libx264 "/content/final.mp4"  # Replace with actual output file name

